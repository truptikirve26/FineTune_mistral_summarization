# ğŸ§  Fine-Tuning Mistral-7B with QLoRA on SAMSum ğŸ“„â¡ï¸ğŸ§¾

This project demonstrates how to fine-tune the [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) model on the SAMSum dataset for dialogue summarization using **QLoRA** â€” a memory-efficient method for adapting large language models using low-rank adapters and 4-bit quantization.

---

## ğŸ” What You'll Learn

- âœ… How to load and preprocess the `samsum` dataset
- âœ… How to quantize a large model using `bitsandbytes`
- âœ… How to apply **QLoRA** (4-bit + Low-Rank Adaptation)
- âœ… How to train the model using Hugging Face's `SFTTrainer`
- âœ… How to evaluate and run sample summarizations

## ğŸ› ï¸ Environment Setup

### 1. Ensure GPU Access (Required)
This tutorial **requires a CUDA-enabled GPU**. Check your setup:

```bash
nvidia-smi


